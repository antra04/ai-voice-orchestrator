{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "acd0cecbb50b4ac0beb8d500c177091d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a59ef710508440eabefab3a6d182a7f6",
              "IPY_MODEL_38d4d8ca46a843c29fcb88b7e6b979c8",
              "IPY_MODEL_9241f729002f438187dd61728ade63c4"
            ],
            "layout": "IPY_MODEL_9dc056313a334630b968ef4f89108678"
          }
        },
        "a59ef710508440eabefab3a6d182a7f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4878da760c1d41509fcf117817e2e013",
            "placeholder": "​",
            "style": "IPY_MODEL_54c235a2e5ff4264bce7d3de46cd0cd2",
            "value": "Loading weights: 100%"
          }
        },
        "38d4d8ca46a843c29fcb88b7e6b979c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41438d94f57a43f3bf0f7967401e7658",
            "max": 148,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2a16fdcf06644158502c6c909ed93e6",
            "value": 148
          }
        },
        "9241f729002f438187dd61728ade63c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f154cfd0e92640548c949bce24d072e0",
            "placeholder": "​",
            "style": "IPY_MODEL_a4358a5df94046608ea43a4fdd35ec3e",
            "value": " 148/148 [00:01&lt;00:00, 103.70it/s, Materializing param=model.layers.15.operator_norm.weight]"
          }
        },
        "9dc056313a334630b968ef4f89108678": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4878da760c1d41509fcf117817e2e013": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54c235a2e5ff4264bce7d3de46cd0cd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41438d94f57a43f3bf0f7967401e7658": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2a16fdcf06644158502c6c909ed93e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f154cfd0e92640548c949bce24d072e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4358a5df94046608ea43a4fdd35ec3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/huggingface/transformers.git@main\"])\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"git+https://github.com/huggingface/accelerate\"])\n",
        "\n",
        "print(\"Latest transformers installed from GitHub\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "szmsL-6B4BvU",
        "outputId": "aef50f95-b683-493b-d4cb-6e087d05cb2a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Latest transformers installed from GitHub\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "\n",
        "MODEL_ID = \"LiquidAI/LFM2.5-1.2B-Instruct\"\n",
        "\n",
        "print(\"Loading LFM2.5 model...\")\n",
        "print(f\"Model ID: {MODEL_ID}\")\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"Device: cuda\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_ID,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "if tokenizer.pad_token_id is None and tokenizer.eos_token_id is not None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "model.eval()\n",
        "\n",
        "print(\"Model loaded\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "\n",
        "# Smoke test with manual prompt format\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Say hello\"}\n",
        "]\n",
        "\n",
        "# Build prompt manually\n",
        "prompt_text = f\"System: {messages[0]['content']}\\nUser: {messages[1]['content']}\\nAssistant:\"\n",
        "\n",
        "inputs = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "input_ids = inputs[\"input_ids\"].to(model.device)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_new_tokens=50,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(f\"\\nTest output: {decoded}\")\n",
        "print(\"\\nLFM2.5 working!\")\n",
        "\n",
        "# Set global variables for next cells\n",
        "tok = tokenizer\n",
        "DEVICE = str(next(model.parameters()).device)\n",
        "DTYPE = torch.bfloat16\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "acd0cecbb50b4ac0beb8d500c177091d",
            "a59ef710508440eabefab3a6d182a7f6",
            "38d4d8ca46a843c29fcb88b7e6b979c8",
            "9241f729002f438187dd61728ade63c4",
            "9dc056313a334630b968ef4f89108678",
            "4878da760c1d41509fcf117817e2e013",
            "54c235a2e5ff4264bce7d3de46cd0cd2",
            "41438d94f57a43f3bf0f7967401e7658",
            "c2a16fdcf06644158502c6c909ed93e6",
            "f154cfd0e92640548c949bce24d072e0",
            "a4358a5df94046608ea43a4fdd35ec3e"
          ]
        },
        "id": "2RxbKaUG7d2Z",
        "outputId": "b1340215-1ef2-41d7-ec60-5541d61e8c8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading LFM2.5 model...\n",
            "Model ID: LiquidAI/LFM2.5-1.2B-Instruct\n",
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acd0cecbb50b4ac0beb8d500c177091d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "WARNING:huggingface_hub.utils._http:Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded\n",
            "Model device: cuda:0\n",
            "\n",
            "Test output: System: You are a helpful assistant.\n",
            "User: Say hello\n",
            "Assistant: Hello! How can I assist you today?\n",
            "\n",
            "LFM2.5 working!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_intent(user_text: str) -> str:\n",
        "    \"\"\"Detect user intent from text\"\"\"\n",
        "    text_lower = (user_text or \"\").lower()\n",
        "\n",
        "    # Greeting intents\n",
        "    greeting_words = [\n",
        "        \"hello\", \"hi\", \"hey\", \"helo\", \"hlw\",\n",
        "        \"namaste\", \"namaskar\",\n",
        "        \"नमस्ते\", \"नमस्कार\", \"हलो\", \"हेलो\",\n",
        "        \"how are you\", \"हाव आर यू\", \"हाउ आर यू\",\n",
        "        \"कैसे हैं\", \"कैसे हो\"\n",
        "    ]\n",
        "    # Only greeting if JUST greeting, not if asking question\n",
        "    if any(word in text_lower for word in greeting_words) and not any(q in text_lower for q in [\"where\", \"वेर\", \"कहाँ\", \"कहां\", \"order\", \"ऑर्डर\"]):\n",
        "        return \"greeting\"\n",
        "\n",
        "    # Order tracking (NEW - check BEFORE refund)\n",
        "    elif any(word in text_lower for word in [\"where\", \"वेर\", \"कहाँ\", \"कहां\", \"order\", \"ऑर्डर\", \"आर्डर\", \"track\", \"status\", \"delivery\"]):\n",
        "        return \"ordertrack\"\n",
        "\n",
        "    # Farewell intents\n",
        "    elif any(word in text_lower for word in [\"bye\", \"goodbye\", \"tata\", \"alvida\", \"thank\", \"thanks\", \"dhanyavaad\", \"धन्यवाद\", \"बाय\", \"थैंक\"]):\n",
        "        return \"farewell\"\n",
        "\n",
        "    # Refund intents\n",
        "    elif any(word in text_lower for word in [\"refund\", \"return\", \"complaint\", \"रिफंड\", \"वापस\", \"रिटर्न\", \"शिकायत\", \"wapas\", \"shikayat\"]):\n",
        "        return \"refundreturn\"\n",
        "\n",
        "    # Login/technical intents\n",
        "    elif any(word in text_lower for word in [\"login\", \"password\", \"crash\", \"error\", \"bug\", \"technical\", \"लॉगिन\", \"पासवर्ड\"]):\n",
        "        return \"logincrash\"\n",
        "\n",
        "    # Sales intents\n",
        "    elif any(word in text_lower for word in [\"buy\", \"purchase\", \"price\", \"cost\", \"discount\", \"offer\", \"kitna\", \"khareed\", \"खरीद\", \"कीमत\"]):\n",
        "        return \"sales\"\n",
        "\n",
        "    # Booking intents\n",
        "    elif any(word in text_lower for word in [\"book\", \"appointment\", \"schedule\", \"reserve\", \"booking\", \"बुकिंग\", \"अपॉइंटमेंट\"]):\n",
        "        return \"booking\"\n",
        "\n",
        "    # General help\n",
        "    elif any(word in text_lower for word in [\"help\", \"मदद\", \"madad\"]):\n",
        "        return \"general\"\n",
        "\n",
        "    # General/unknown\n",
        "    else:\n",
        "        return \"general\"\n",
        "\n",
        "print(\"Intent detection with order tracking support\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKcswdef4hnn",
        "outputId": "de4cf884-4f50-4081-e1db-272977c0a74e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intent detection with order tracking support\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Templates for Hindi and Hinglish responses\n",
        "TEMPLATES = {\n",
        "    \"hindi\": {\n",
        "        \"greeting\": \"नमस्ते। मैं आपकी कैसे मदद कर सकता हूं?\",\n",
        "        \"farewell\": \"धन्यवाद। आपका दिन शुभ हो।\",\n",
        "        \"refund/return\": \"मैं आपकी समस्या समझ रहा हूं। कृपया मुझे अपना ऑर्डर नंबर बताएं।\",\n",
        "        \"login/crash\": \"मैं इस तकनीकी समस्या में आपकी मदद करूंगा।\",\n",
        "        \"sales\": \"हमारे पास बेहतरीन ऑफर्स हैं। आप किस प्रोडक्ट में रुचि रखते हैं?\",\n",
        "        \"booking\": \"मैं आपके लिए अपॉइंटमेंट बुक कर सकता हूं। कौन सी तारीख सही रहेगी?\",\n",
        "        \"general\": \"जी हां, मैं आपकी मदद करूंगा। कृपया अपनी समस्या बताएं।\",\n",
        "        \"unknown\": \"क्षमा करें, मैं समझ नहीं पाया। क्या आप फिर से बता सकते हैं?\"\n",
        "    },\n",
        "    \"hinglish\": {\n",
        "        \"greeting\": \"Hello! Main aapki kaise help kar sakta hoon?\",\n",
        "        \"farewell\": \"Thank you! Aapka din accha rahe.\",\n",
        "        \"refund/return\": \"Main aapki problem samajh raha hoon. Please order number batayein.\",\n",
        "        \"login/crash\": \"Main is technical issue me aapki help karunga.\",\n",
        "        \"sales\": \"Hamare paas bahut acche offers hain. Aap kis product me interested hain?\",\n",
        "        \"booking\": \"Main appointment book kar sakta hoon. Konsi date theek rahegi?\",\n",
        "        \"general\": \"Ji haan, main aapki help karunga. Apni problem batayein.\",\n",
        "        \"unknown\": \"Sorry, main samjha nahi. Dobara bata sakte hain?\"\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"Templates configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6nm4KUY6ALQ",
        "outputId": "9797314e-1b39-4f96-fab7-07e714058b70"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Templates configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_hindi_reply(user_text: str, intent: str, persona_key: str) -> str:\n",
        "    \"\"\"Generate response in Hindi\"\"\"\n",
        "    device = str(next(model.parameters()).device)\n",
        "\n",
        "    # Create Hindi-aware system prompt\n",
        "    sys_prompt = f\"\"\"You are a helpful Hindi-speaking customer service agent.\n",
        "Rules:\n",
        "- Respond ONLY in Hindi (Devanagari script)\n",
        "- Keep responses short (1-2 sentences maximum)\n",
        "- Be polite and professional\n",
        "- If it's a greeting, greet back\n",
        "- If it's a complaint, apologize and help\n",
        "- If it's a question, answer briefly\n",
        "\n",
        "Customer said: {user_text}\n",
        "\n",
        "Respond in Hindi:\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful customer service agent who speaks Hindi.\"},\n",
        "        {\"role\": \"user\", \"content\": sys_prompt},\n",
        "    ]\n",
        "\n",
        "    inputs = build_inputs_for_chat(tok, messages, device)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.3,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "        )\n",
        "\n",
        "    new_ids = out_ids[0][input_len:]\n",
        "    text = tok.decode(new_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # If LLM fails to generate Hindi, use template\n",
        "    if not text or not has_devanagari(text):\n",
        "        return TEMPLATES[\"hindi\"].get(intent, TEMPLATES[\"hindi\"][\"general\"])\n",
        "\n",
        "    return limit_3_sentences(text)\n",
        "\n",
        "\n",
        "def generate_hinglish_reply(user_text: str, intent: str, persona_key: str) -> str:\n",
        "    \"\"\"Generate response in Hinglish (Hindi + English mix)\"\"\"\n",
        "    device = str(next(model.parameters()).device)\n",
        "\n",
        "    sys_prompt = f\"\"\"You are a helpful customer service agent who speaks Hinglish (mix of Hindi and English).\n",
        "Rules:\n",
        "- Respond in Hinglish (Roman script with Hindi words mixed with English)\n",
        "- Keep responses short (1-2 sentences)\n",
        "- Be friendly and professional\n",
        "- Example: \"Ji haan, main aapki help karunga. Aap apna order number bataiye.\"\n",
        "\n",
        "Customer said: {user_text}\n",
        "\n",
        "Respond in Hinglish:\"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You speak Hinglish (Hindi-English mix).\"},\n",
        "        {\"role\": \"user\", \"content\": sys_prompt},\n",
        "    ]\n",
        "\n",
        "    inputs = build_inputs_for_chat(tok, messages, device)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=50,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.3,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "        )\n",
        "\n",
        "    new_ids = out_ids[0][input_len:]\n",
        "    text = tok.decode(new_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # If LLM fails, use template\n",
        "    if not text:\n",
        "        return TEMPLATES[\"hinglish\"].get(intent, TEMPLATES[\"hinglish\"][\"general\"])\n",
        "\n",
        "    return limit_3_sentences(text)\n",
        "\n",
        "print(\"Hindi and Hinglish generation functions configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDCOWNrbWln6",
        "outputId": "da658b69-21f3-411a-b6ad-881be3cfec27"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hindi and Hinglish generation functions configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Improved Personas following the standard template\n",
        "PERSONAS = {\n",
        "    \"1. Customer Care (General)\": {\n",
        "        \"role\": \"Customer Care Agent\",\n",
        "        \"objective\": \"Handle customer inquiries, resolve issues, and provide support\",\n",
        "        \"tone\": \"polite, calm, empathetic\",\n",
        "        \"rules\": [\n",
        "            \"If customer is angry, respond calmly and acknowledge their frustration\",\n",
        "            \"If issue is unclear, ask one clear question at a time\",\n",
        "            \"If policy restricted, say 'I'll help you with the best available option'\",\n",
        "            \"Never argue with customer\"\n",
        "        ]\n",
        "    },\n",
        "    \"2. Sales Lead Qualification\": {\n",
        "        \"role\": \"Sales Representative\",\n",
        "        \"objective\": \"Identify customer interest and qualify leads\",\n",
        "        \"tone\": \"confident, persuasive but not pushy\",\n",
        "        \"rules\": [\n",
        "            \"No false promises\",\n",
        "            \"No pricing unless allowed\",\n",
        "            \"Avoid aggressive language\",\n",
        "            \"Escalate to human if lead is hot\"\n",
        "        ]\n",
        "    },\n",
        "    \"3. Appointment Booking\": {\n",
        "        \"role\": \"Booking Assistant\",\n",
        "        \"objective\": \"Schedule appointments and confirm bookings\",\n",
        "        \"tone\": \"structured, clear, time-aware\",\n",
        "        \"rules\": [\n",
        "            \"Collect date and time\",\n",
        "            \"Confirm availability\",\n",
        "            \"Repeat details for confirmation\"\n",
        "        ]\n",
        "    },\n",
        "    \"4. Complaint Handling\": {\n",
        "        \"role\": \"Complaint Resolution Agent\",\n",
        "        \"objective\": \"Handle refunds, escalations, and dissatisfaction\",\n",
        "        \"tone\": \"highly empathetic, patient\",\n",
        "        \"rules\": [\n",
        "            \"Acknowledge frustration first\",\n",
        "            \"Apologize where applicable\",\n",
        "            \"Explain next steps clearly\",\n",
        "            \"Never argue with customer\"\n",
        "        ]\n",
        "    },\n",
        "    \"5. General Information Assistant\": {\n",
        "        \"role\": \"Information Assistant\",\n",
        "        \"objective\": \"Answer FAQs and provide business information\",\n",
        "        \"tone\": \"neutral, informative\",\n",
        "        \"rules\": [\n",
        "            \"Answer factual questions only\",\n",
        "            \"Redirect to support if needed\"\n",
        "        ]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Language detection utilities\n",
        "DEVANAGARI_RE = re.compile(r'[\\u0900-\\u097F]')\n",
        "LATIN_RE = re.compile(r'[A-Za-z]')\n",
        "\n",
        "def has_devanagari(s: str) -> bool:\n",
        "    return bool(DEVANAGARI_RE.search(s or \"\"))\n",
        "\n",
        "def has_latin(s: str) -> bool:\n",
        "    return bool(LATIN_RE.search(s or \"\"))\n",
        "\n",
        "def clean_tts(s: str) -> str:\n",
        "    return re.sub(r\"[\\\"']\", \"\", s or \"\").strip()\n",
        "\n",
        "def limit_3_sentences(text: str) -> str:\n",
        "    t = clean_tts(text)\n",
        "    if not t:\n",
        "        return t\n",
        "    parts = re.split(r'[.?!।]+', t)\n",
        "    parts = [p.strip() for p in parts if p.strip()]\n",
        "    return \". \".join(parts[:3]).strip()\n",
        "\n",
        "ROMAN_HI_HINTS = {\n",
        "    \"aap\",\"ap\",\"tum\",\"tu\",\"mera\",\"meri\",\"mere\",\"kya\",\"kyu\",\"kyun\",\"kaise\",\"kab\",\"kahan\",\n",
        "    \"nahi\",\"nahin\",\"haan\",\"han\",\"hai\",\"ho\",\"hu\",\"hun\",\"thoda\",\"jaldi\",\"abhi\",\n",
        "    \"karo\",\"kar\",\"karna\",\"karne\",\"kardo\",\"krdo\",\"batao\",\"batado\",\"bataye\",\"dijiye\",\"do\",\"de\"\n",
        "}\n",
        "\n",
        "def detect_mode(user_text: str) -> str:\n",
        "    \"\"\"Detect language mode\"\"\"\n",
        "    text = (user_text or \"\").strip()\n",
        "    if not text:\n",
        "        return \"english\"\n",
        "\n",
        "    has_dev = has_devanagari(text)\n",
        "    has_lat = has_latin(text)\n",
        "\n",
        "    # If has both Devanagari and Latin, it's Hinglish\n",
        "    if has_dev and has_lat:\n",
        "        return \"hinglish\"\n",
        "\n",
        "    # If ONLY Devanagari\n",
        "    if has_dev and not has_lat:\n",
        "        # Expanded list of English words written in Devanagari\n",
        "        hinglish_devanagari = [\n",
        "            \"हाय\", \"हाव\", \"हाउ\", \"आर\", \"यू\", \"हलो\", \"हेलो\",\n",
        "            \"बाय\", \"थैंक\", \"प्लीज\", \"ओके\",\n",
        "            \"वेर\", \"इज\", \"माई\", \"योर\",  # where, is, my, your\n",
        "            \"ऑर्डर\", \"आर्डर\"  # order\n",
        "        ]\n",
        "\n",
        "        # Count Hinglish words\n",
        "        words = text.split()\n",
        "        hinglish_count = sum(1 for word in words if any(hw in word for hw in hinglish_devanagari))\n",
        "\n",
        "        # If more than 30% are Hinglish words → Hinglish\n",
        "        if len(words) > 0 and hinglish_count / len(words) > 0.3:\n",
        "            return \"hinglish\"\n",
        "\n",
        "        return \"hindi\"\n",
        "\n",
        "    # If has Latin, check for Hindi hints\n",
        "    tokens = re.findall(r'[A-Za-z]+', text.lower())\n",
        "    hint_count = sum(1 for t in tokens if t in ROMAN_HI_HINTS)\n",
        "    if hint_count >= 1:\n",
        "        return \"hinglish\"\n",
        "\n",
        "    return \"english\"\n",
        "\n",
        "print(\"Improved language detection - better Hinglish recognition\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccQUZqy19Fl4",
        "outputId": "cd985bef-888f-4ba1-9e87-c330091e667a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Improved language detection - better Hinglish recognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_english_system_prompt(persona_key: str) -> str:\n",
        "    \"\"\"Build multilingual system prompt\"\"\"\n",
        "    persona = PERSONAS[persona_key]\n",
        "    role = persona[\"role\"]\n",
        "    objective = persona[\"objective\"]\n",
        "    tone = persona[\"tone\"]\n",
        "    rules = persona[\"rules\"]\n",
        "\n",
        "    rules_text = \"\\n\".join([f\"- {rule}\" for rule in rules])\n",
        "\n",
        "    prompt = f\"\"\"You are an AI voice assistant acting as {role}.\n",
        "\n",
        "Your purpose:\n",
        "- {objective}\n",
        "\n",
        "IMPORTANT LANGUAGE RULES:\n",
        "- If user speaks HINDI (Devanagari script), respond ONLY in HINDI\n",
        "- If user speaks HINGLISH (Roman Hindi), respond in HINGLISH\n",
        "- If user speaks ENGLISH, respond in ENGLISH\n",
        "- Match the user's language exactly\n",
        "\n",
        "Communication style:\n",
        "- Tone: {tone}\n",
        "- Sentence length: short, clear, conversational (1-2 sentences maximum)\n",
        "\n",
        "Rules:\n",
        "{rules_text}\n",
        "- Do not hallucinate information\n",
        "- Never mention AI, model, or system\n",
        "- Keep responses under 3 sentences\n",
        "\n",
        "Response format:\n",
        "- Voice-friendly (no bullet points)\n",
        "- No emojis\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def build_inputs_for_chat(tokenizer, messages, device: str):\n",
        "    \"\"\"Build tokenized inputs for chat\"\"\"\n",
        "    # Manual prompt format for Liquid AI\n",
        "    lines = []\n",
        "    for m in messages:\n",
        "        role = (m.get(\"role\") or \"user\").strip().capitalize()\n",
        "        content = (m.get(\"content\") or \"\").strip()\n",
        "        lines.append(f\"{role}: {content}\")\n",
        "    lines.append(\"Assistant:\")\n",
        "\n",
        "    prompt_text = \"\\n\".join(lines)\n",
        "    enc = tokenizer(prompt_text, return_tensors=\"pt\")\n",
        "    return {k: v.to(device) for k, v in enc.items()}\n",
        "\n",
        "\n",
        "def generate_english_reply(user_text: str, persona_key: str) -> str:\n",
        "    \"\"\"Generate multilingual response using Liquid model\"\"\"\n",
        "    device = str(next(model.parameters()).device)\n",
        "    sys_prompt = build_english_system_prompt(persona_key)\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_prompt},\n",
        "        {\"role\": \"user\", \"content\": (user_text or \"\").strip()},\n",
        "    ]\n",
        "\n",
        "    inputs = build_inputs_for_chat(tok, messages, device)\n",
        "    input_len = inputs[\"input_ids\"].shape[-1]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=80,        # Increased for Hindi (needs more tokens)\n",
        "            do_sample=True,\n",
        "            temperature=0.8,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.3,\n",
        "            no_repeat_ngram_size=3,\n",
        "            pad_token_id=tok.pad_token_id,\n",
        "            eos_token_id=tok.eos_token_id,\n",
        "        )\n",
        "\n",
        "    new_ids = out_ids[0][input_len:]\n",
        "    text = tok.decode(new_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Clean up response but keep multilingual content\n",
        "    text = limit_3_sentences(text)\n",
        "\n",
        "    if not text:\n",
        "        text = \"I understand. Could you please provide more details?\"\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "print(\" Multilingual generation configured - supports English, Hindi, Hinglish\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUqKWyQC9VPO",
        "outputId": "f3b86a10-d41a-498b-b237-993db32ca825"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Multilingual generation configured - supports English, Hindi, Hinglish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def liquidbrain_reply(user_text: str, persona_key: str = \"1. Customer Care (General)\"):\n",
        "    \"\"\"\n",
        "    Main function to generate reply\n",
        "    Returns: (reply_text, detected_mode, detected_intent)\n",
        "    \"\"\"\n",
        "    detected_mode = detect_mode(user_text)\n",
        "    intent = detect_intent(user_text)\n",
        "\n",
        "    print(f\"DEBUG: Mode={detected_mode}, Intent={intent}, Input={user_text}\")\n",
        "\n",
        "    # Response templates by language\n",
        "    RESPONSE_TEMPLATES = {\n",
        "        \"hindi\": {\n",
        "            \"greeting\": \"नमस्ते। मैं आपकी कैसे मदद कर सकता हूं?\",\n",
        "            \"farewell\": \"धन्यवाद। आपका दिन शुभ रहे।\",\n",
        "            \"ordertrack\": \"मैं आपके ऑर्डर की जानकारी देखता हूं। कृपया ऑर्डर नंबर बताएं।\",\n",
        "            \"refundreturn\": \"मैं आपके रिफंड में मदद करूंगा। कृपया ऑर्डर नंबर बताएं।\",\n",
        "            \"logincrash\": \"मैं इस तकनीकी समस्या में आपकी मदद करूंगा। कृपया विवरण बताएं।\",\n",
        "            \"sales\": \"हमारे पास बहुत अच्छे ऑफर हैं। आप किस उत्पाद में रुचि रखते हैं?\",\n",
        "            \"booking\": \"मैं आपकी अपॉइंटमेंट बुक कर सकता हूं। कौन सी तारीख ठीक रहेगी?\",\n",
        "            \"general\": \"मैं आपकी मदद करूंगा। कृपया अपनी समस्या बताएं।\",\n",
        "        },\n",
        "        \"hinglish\": {\n",
        "            \"greeting\": \"Hello! Main aapki kaise help kar sakta hoon?\",\n",
        "            \"farewell\": \"Thank you! Aapka din accha rahe.\",\n",
        "            \"ordertrack\": \"Main aapke order ki details check karta hoon. Order number bataiye please.\",\n",
        "            \"refundreturn\": \"Main aapki refund me madad karunga. Order number bataiye please.\",\n",
        "            \"logincrash\": \"Main is technical issue me aapki help karunga. Problem detail me bataiye.\",\n",
        "            \"sales\": \"Hamare paas bahut acche offers hain. Aap kis product me interested hain?\",\n",
        "            \"booking\": \"Main appointment book kar sakta hoon. Konsi date theek rahegi?\",\n",
        "            \"general\": \"Ji haan, main aapki help karunga. Apni problem bataiye.\",\n",
        "        },\n",
        "        \"english\": {\n",
        "            \"greeting\": \"Hello! How can I help you today?\",\n",
        "            \"farewell\": \"Thank you for contacting us. Have a great day!\",\n",
        "            \"ordertrack\": \"I'll check your order details. Please provide your order number.\",\n",
        "            \"refundreturn\": \"I'll help you with the refund. Please share your order number.\",\n",
        "            \"logincrash\": \"I'll assist you with this technical issue. Please describe the problem in detail.\",\n",
        "            \"sales\": \"We have great offers available. Which product are you interested in?\",\n",
        "            \"booking\": \"I can book an appointment for you. Which date works best?\",\n",
        "            \"general\": \"I'm here to help. Please tell me what you need assistance with.\",\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Get response based on detected mode\n",
        "    if detected_mode in [\"hindi\", \"hinglish\"]:\n",
        "        # Use templates for Hindi/Hinglish (faster, more consistent)\n",
        "        responses = RESPONSE_TEMPLATES.get(detected_mode, RESPONSE_TEMPLATES[\"english\"])\n",
        "        final_text = responses.get(intent, responses[\"general\"])\n",
        "    else:\n",
        "        # Use templates for English too (faster than LLM)\n",
        "        responses = RESPONSE_TEMPLATES[\"english\"]\n",
        "        final_text = responses.get(intent, responses[\"general\"])\n",
        "\n",
        "    final_text = clean_tts(final_text)\n",
        "    return final_text, detected_mode, intent\n",
        "\n",
        "\n",
        "print(\" Liquid Brain reply function - Optimized with English templates\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH_Jug019Zpy",
        "outputId": "4b80cd18-8da7-40b9-d059-35afee59c99d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Liquid Brain reply function - Optimized with English templates\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_numbers_to_words(text, language=\"hindi\"):\n",
        "    \"\"\"Convert numbers to words in text based on language\"\"\"\n",
        "    import re\n",
        "\n",
        "    # Digit to word mappings\n",
        "    digit_maps = {\n",
        "        \"hindi\": {\n",
        "            \"0\": \"शून्य\", \"1\": \"एक\", \"2\": \"दो\", \"3\": \"तीन\", \"4\": \"चार\",\n",
        "            \"5\": \"पांच\", \"6\": \"छह\", \"7\": \"सात\", \"8\": \"आठ\", \"9\": \"नौ\"\n",
        "        },\n",
        "        \"english\": {\n",
        "            \"0\": \"zero\", \"1\": \"one\", \"2\": \"two\", \"3\": \"three\", \"4\": \"four\",\n",
        "            \"5\": \"five\", \"6\": \"six\", \"7\": \"seven\", \"8\": \"eight\", \"9\": \"nine\"\n",
        "        },\n",
        "        \"hinglish\": {\n",
        "            \"0\": \"zero\", \"1\": \"ek\", \"2\": \"do\", \"3\": \"teen\", \"4\": \"char\",\n",
        "            \"5\": \"paanch\", \"6\": \"chhe\", \"7\": \"saat\", \"8\": \"aath\", \"9\": \"nau\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    digit_map = digit_maps.get(language, digit_maps[\"english\"])\n",
        "\n",
        "    def replace_number(match):\n",
        "        number = match.group(0)\n",
        "        # Convert each digit separately\n",
        "        words = \" \".join([digit_map.get(d, d) for d in number])\n",
        "        return words\n",
        "\n",
        "    # Replace all number sequences with words\n",
        "    text = re.sub(r'\\d+', replace_number, text)\n",
        "\n",
        "    return text\n",
        "\n",
        "print(\"Number to words converter configured\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07Hicd57Ckwc",
        "outputId": "7ad0f58c-fe7b-46a2-b9d9-a2dde46eb703"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number to words converter configured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q fastapi uvicorn nest-asyncio pyngrok python-multipart\n"
      ],
      "metadata": {
        "id": "23VcVBOI9cnI"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "from pydantic import BaseModel\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "from threading import Thread\n",
        "import getpass\n",
        "\n",
        "nest_asyncio.apply()\n"
      ],
      "metadata": {
        "id": "gahLNc9_9okj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Voice mapping matching TTS capabilities\n",
        "VOICE_MAP = {\n",
        "    # Hindi speakers (Rohit removed)\n",
        "    \"hindi_rani\": \"Rani speaks in a formal, polished tone with precise pronunciation and professional composure suitable for corporate calls\",\n",
        "    \"hindi_divya\": \"Divya speaks in a formal, polished tone with precise pronunciation and professional composure suitable for corporate calls\",\n",
        "    \"hindi_aman\": \"Aman speaks in a formal, polished tone with precise pronunciation and professional composure suitable for corporate calls\",\n",
        "\n",
        "    # English speakers\n",
        "    \"english_mary\": \"Mary speaks in a formal, polished tone with precise pronunciation and professional composure suitable for corporate calls\",\n",
        "    \"english_thoma\": \"Thoma speaks in a formal, polished tone with precise pronunciation and professional composure suitable for corporate calls\",\n",
        "    \"english_swapna\": \"Swapna speaks in a formal, polished tone with precise pronunciation and professional composure suitable for corporate calls\",\n",
        "    \"english_dinesh\": \"Dinesh speaks in a formal, polished tone with precise pronunciation and professional composure suitable for corporate calls\",\n",
        "    \"english_meera\": \"Meera speaks in a formal, polished tone with precise pronunciation and professional composure suitable for corporate calls\",\n",
        "    \"english_jatin\": \"Jatin speaks in a formal, polished tone with precise pronunciation and professional composure suitable for corporate calls\",\n",
        "}\n",
        "\n",
        "def select_voice_description(detected_mode, intent):\n",
        "    \"\"\"Select appropriate voice based on language\"\"\"\n",
        "    import random\n",
        "\n",
        "    if detected_mode == \"hindi\" or detected_mode == \"hinglish\":\n",
        "        # Use Hindi speakers\n",
        "        hindi_voices = [\n",
        "            VOICE_MAP[\"hindi_rani\"],\n",
        "            VOICE_MAP[\"hindi_divya\"],\n",
        "            VOICE_MAP[\"hindi_aman\"]\n",
        "        ]\n",
        "        return random.choice(hindi_voices)\n",
        "    else:\n",
        "        # Use English speakers for English\n",
        "        english_voices = [\n",
        "            VOICE_MAP[\"english_mary\"],\n",
        "            VOICE_MAP[\"english_thoma\"],\n",
        "            VOICE_MAP[\"english_swapna\"],\n",
        "            VOICE_MAP[\"english_dinesh\"],\n",
        "            VOICE_MAP[\"english_meera\"],\n",
        "            VOICE_MAP[\"english_jatin\"]\n",
        "        ]\n",
        "        return random.choice(english_voices)\n",
        "\n",
        "print(\"Voice mapping \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUG7zgfx9rZ6",
        "outputId": "33d56575-477b-4311-b2cd-df0fb8e7d1ab"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voice mapping - Rohit removed, only Rani, Divya, Aman for Hindi/Hinglish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "\n",
        "app = FastAPI(\n",
        "    title=\"Liquid AI LLM API\",\n",
        "    description=\"Text from STT → Reply + Voice Description for TTS\",\n",
        "    version=\"2.0\"\n",
        ")\n",
        "\n",
        "class TTSInput(BaseModel):\n",
        "    text: str\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root():\n",
        "    return {\n",
        "        \"status\": \"online\",\n",
        "        \"service\": \"Liquid AI LLM\",\n",
        "        \"model\": MODEL_ID\n",
        "    }\n",
        "\n",
        "@app.post(\"/generate\")\n",
        "def generate_response(input: TTSInput):\n",
        "    \"\"\"STT text → LLM reply with TTS description\"\"\"\n",
        "    try:\n",
        "        # Generate reply using LLM\n",
        "        reply_text, detected_mode, detected_intent = liquidbrain_reply(input.text)\n",
        "\n",
        "        # Convert numbers to words for TTS compatibility\n",
        "        reply_text = convert_numbers_to_words(reply_text, detected_mode)\n",
        "\n",
        "        # Select voice description using the configured function\n",
        "        description = select_voice_description(detected_mode, detected_intent)\n",
        "\n",
        "        print(f\"Mode: {detected_mode}, Intent: {detected_intent}\")\n",
        "        print(f\"Voice: {description[:50]}...\")\n",
        "\n",
        "        return {\n",
        "            \"text\": reply_text,\n",
        "            \"description\": description\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health():\n",
        "    return {\"status\": \"healthy\", \"model\": MODEL_ID, \"device\": str(DEVICE)}\n",
        "\n",
        "print(\"FastAPI app created with voice selection \")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSe6gruf9-cv",
        "outputId": "a54bc714-b9d5-4b11-e706-6285c2730fd6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastAPI app created with voice selection - Rohit removed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "import time\n",
        "from threading import Thread\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "\n",
        "\n",
        "# Close existing ngrok tunnels\n",
        "try:\n",
        "    ngrok.kill()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Start server on port 8003\n",
        "def run_server():\n",
        "    uvicorn.run(\n",
        "        app,\n",
        "        host=\"0.0.0.0\",\n",
        "        port=8003,\n",
        "        log_level=\"info\"\n",
        "    )\n",
        "\n",
        "server_thread = Thread(target=run_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "print(\"Starting server...\")\n",
        "time.sleep(3)\n",
        "\n",
        "# Setup ngrok tunnel\n",
        "token = getpass.getpass(\"Enter your ngrok auth token: \")\n",
        "ngrok.set_auth_token(token)\n",
        "public_url = ngrok.connect(8003, bind_tls=True)\n",
        "\n",
        "print(\"Liquid AI LLM API is now live\")\n",
        "print(f\"Public URL: {public_url}\")\n",
        "print(f\"API Documentation: {public_url}/docs\")\n",
        "print(f\"Health Check: {public_url}/health\")\n",
        "print(\"\\nEndpoint for orchestration:\")\n",
        "print(f\"POST {public_url}/generate\")\n",
        "print('Request body: {\"text\": \"user text from STT\"}')\n",
        "print(\"\\nResponse: {\\\"text\\\": \\\"reply\\\", \\\"description\\\": \\\"voice description\\\"}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vda6wKdZBpqH",
        "outputId": "894a201b-81f4-4748-aee2-f2a400c51f20"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting server...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [28135]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "ERROR:    [Errno 98] error while attempting to bind on address ('0.0.0.0', 8003): address already in use\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your ngrok auth token: ··········\n",
            "Liquid AI LLM API is now live\n",
            "Public URL: NgrokTunnel: \"https://unhealable-jonell-intermittingly.ngrok-free.dev\" -> \"http://localhost:8003\"\n",
            "API Documentation: NgrokTunnel: \"https://unhealable-jonell-intermittingly.ngrok-free.dev\" -> \"http://localhost:8003\"/docs\n",
            "Health Check: NgrokTunnel: \"https://unhealable-jonell-intermittingly.ngrok-free.dev\" -> \"http://localhost:8003\"/health\n",
            "\n",
            "Endpoint for orchestration:\n",
            "POST NgrokTunnel: \"https://unhealable-jonell-intermittingly.ngrok-free.dev\" -> \"http://localhost:8003\"/generate\n",
            "Request body: {\"text\": \"user text from STT\"}\n",
            "\n",
            "Response: {\"text\": \"reply\", \"description\": \"voice description\"}\n"
          ]
        }
      ]
    }
  ]
}